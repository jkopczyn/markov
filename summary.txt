MarkovState: tracks moment-to-moment of running generator
	__init__: self.markov is a chain/data store, 
		self.generator is a function with state
		both start null.
	load:	take filename, create a Markov instance, 
		save it to self.markov, 
		load the file into that instance
	dump:	take filename, save the Markov self.markov to that file
	train:	take n (prefix length), stream (file or STDIN),
			and setting for noparagraphs
		Turn the file into tokens, 
		create Markov instance of prefix-length n,
		save to self.markov and train it on the token stream,
		clear generator
	generate: take chunks (how much to generate), 
			random seed (defaults to systime), 
			prob (chance to choose random token, default 0), 
			offset (number of initial tokens to discard),
			cln (tokens to clear after a clause, default 0),
			startf (lambda for first token to use, default true),
			endchunkf (lambda for last token to use, default true),
			kill (number of tokens to drop off the end),
			prefix (start out with these tokens before generating),
		set seed to systime as int if not defined (and print it),
		reset markov(seed, prob, prefix, cln),
		pass over offset instances of next(self.markov),
		continue until the next token meets the startf predicate,
		define a generator function gen(n)
			out is an empty array
			shove next(self.markov) into out
			if the token satisfies endchunkf, decrement n
			return out joined with ' '
		assign this to self.generator
		return n chunks from the generator
	more: take chunks (default 1) and generate more from the
		pre-existing generator













Markov: stores and structures the data
	__init__: takes n (prefix size), default 3,
		set self.n to n, self.p to 0, self.seed to null,
		self.data to empty hash, self.cln to n
	set_cln: takes cln (tokens to clear after a clause)
	train: takes token stream of training data,
		set prev to empty, iterate through stream as token,
			create a list of suffixes of prev and iterate 
				through it as pprev,
			for each pprev, if it is not in self.data, initialize
				it to [0, {}] (no occurences, no successors),
			then add the current token to the hash of the entry,
				adding it as a key if necessary,
			and increment the total count and token-specific count,
		then (in the each-token loop) append this token to prev,
		remove the head of prev if it's too long,
		and repeat until the token stream is empty
	load: takes filename, deserialize model from filename,
		parse import to get n, self.data (prefix-length, hash),
		set self.n to min (n, self.n) and print if changed,
		do this under try-except for safety
	dump: takes filename, serializes (self, self.data) to filename,
		do this under try-except for safety
	reset: takes seed (new random seed), 
			prob (chance to choose random token), 
			prev (start out with these tokens before generating),
			cln (tokens to clear after a clause),
		set everything to new values,
		initialize the RNG with the seed
	__iter__: return self (iterator of a Markov is itself, it is iterable)
	__next__: pick an key for self.data,
		with probability self.p this will be the empty key (),
		else it will be self.prev,
		set next to self._choose(self.data[that key]),
			 - on any exception retry with ()
		append next to self.prev,
		if self.prev is longer than self.n, drop the head,
		if the end of next is a clause-ender (.?! or \n, depending),
			keep only the last self.cln tokens in self.prev
		return next
	_choose: (private) takes a two-element array freqdict, 
		parses it to total, choices,
			(i.e. sum of all freqs, hash of all successors)
		generate a random idx up to total,
		then index through the choices.items as (token, freq),
		and either return token or subtract freq from idx




Adding in anti-loop condition: the end-of-chunk check is in a different place
from the state, which makes my preferred solution of 'check if the state is
saturated and close out regardless of endchunkf check' difficult. 
'Saturated': record how many times a given token has been picked from a given
state in the course of the current chunk. If it is as many times as that token
appears in the appropriate tokendict, do not pick it again. If every token meets
this criterion, end immediately.
Coarser-grained solution: On a per-chunk basis, track the number of visits to
each state; if this ever exceeds the number of occurences in the training set,
assume a loop has been found and end the chunk.
This has some false positives, which could be a problem for small training sets.
For large sets it has little issues, and guarantees a finite bound (size of the
entire training set) on the longest output generated, and a corresponding time
bound.
